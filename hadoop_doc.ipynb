{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a8a5380fd4c0b5c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üõ†Ô∏è Apache Hadoop 3.3.6 Installation Guide (Ubuntu 20.04)\n",
    "\n",
    "> ‚úÖ Single-node (pseudo-distributed) Hadoop setup with **Java 8**, covering installation, configuration, and **real-world troubleshooting** (like Java 17 errors).\n",
    "\n",
    "---\n",
    "\n",
    "## üìã System Requirements\n",
    "\n",
    "| Component          | Requirement                                     |\n",
    "|--------------------|-------------------------------------------------|\n",
    "| OS                 | Ubuntu 20.04 (tested)                           |\n",
    "| RAM                | 8 GB minimum                                    |\n",
    "| Disk Space         | At least 10‚Äì20 GB (HDFS, logs, temp)            |\n",
    "| Java Version       | ‚úÖ Java 8 (**recommended**)                      |\n",
    "| Hadoop Version     | Apache Hadoop 3.3.6                             |\n",
    "| SSH                | Required for Hadoop daemon coordination         |\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Step 1: Install Dependencies\n",
    "\n",
    "```bash\n",
    "sudo apt update && sudo apt upgrade -y\n",
    "sudo apt install -y ssh rsync curl wget openjdk-8-jdk\n",
    "```\n",
    "\n",
    "‚úÖ **Why?**\n",
    "- Java is needed for Hadoop to run\n",
    "- SSH is required for Hadoop daemons to talk to each other ‚Äî even on localhost\n",
    "\n",
    "---\n",
    "\n",
    "## üìÅ Step 2: Download and Set Up Hadoop\n",
    "\n",
    "```bash\n",
    "HADOOP_VERSION=3.3.6\n",
    "cd ~\n",
    "wget https://downloads.apache.org/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz\n",
    "tar -xzf hadoop-$HADOOP_VERSION.tar.gz\n",
    "mv hadoop-$HADOOP_VERSION hadoop\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Step 3: Set Environment Variables\n",
    "\n",
    "Add this to `~/.bashrc`:\n",
    "\n",
    "```bash\n",
    "export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n",
    "export HADOOP_HOME=$HOME/hadoop\n",
    "export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\n",
    "export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\n",
    "```\n",
    "\n",
    "Then reload:\n",
    "\n",
    "```bash\n",
    "source ~/.bashrc\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Step 4: Configure Hadoop\n",
    "\n",
    "### Edit `hadoop-env.sh`\n",
    "\n",
    "```bash\n",
    "nano $HADOOP_HOME/etc/hadoop/hadoop-env.sh\n",
    "```\n",
    "\n",
    "Set:\n",
    "\n",
    "```bash\n",
    "export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n",
    "```\n",
    "\n",
    "### Edit `core-site.xml`\n",
    "\n",
    "```xml\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>fs.defaultFS</name>\n",
    "    <value>hdfs://localhost:9000</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>hadoop.tmp.dir</name>\n",
    "    <value>/home/neosoft/hadoop_data/tmp</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "### Edit `hdfs-site.xml`\n",
    "\n",
    "```xml\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>dfs.replication</name>\n",
    "    <value>1</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>dfs.namenode.name.dir</name>\n",
    "    <value>file:///home/neosoft/hadoop_data/dfs/namenode</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>dfs.datanode.data.dir</name>\n",
    "    <value>file:///home/neosoft/hadoop_data/dfs/datanode</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "### Edit `mapred-site.xml`\n",
    "\n",
    "```bash\n",
    "cp mapred-site.xml.template mapred-site.xml\n",
    "```\n",
    "\n",
    "```xml\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>mapreduce.framework.name</name>\n",
    "    <value>yarn</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "### Edit `yarn-site.xml`\n",
    "\n",
    "```xml\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>yarn.nodemanager.aux-services</name>\n",
    "    <value>mapreduce_shuffle</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìÇ Step 5: Create Local Hadoop Directories\n",
    "\n",
    "```bash\n",
    "mkdir -p ~/hadoop_data/tmp\n",
    "mkdir -p ~/hadoop_data/dfs/namenode\n",
    "mkdir -p ~/hadoop_data/dfs/datanode\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîê Step 6: Set Up Passwordless SSH\n",
    "\n",
    "```bash\n",
    "ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\n",
    "cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n",
    "chmod 600 ~/.ssh/authorized_keys\n",
    "ssh localhost\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Step 7: Format the NameNode\n",
    "\n",
    "```bash\n",
    "hdfs namenode -format\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Step 8: Start Hadoop Daemons\n",
    "\n",
    "```bash\n",
    "start-dfs.sh\n",
    "start-yarn.sh\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Step 9: Verify Services\n",
    "\n",
    "```bash\n",
    "jps\n",
    "```\n",
    "\n",
    "Should show:\n",
    "```\n",
    "NameNode\n",
    "DataNode\n",
    "SecondaryNameNode\n",
    "ResourceManager\n",
    "NodeManager\n",
    "```\n",
    "\n",
    "UI Access:\n",
    "- NameNode: [http://localhost:9870](http://localhost:9870)\n",
    "- YARN: [http://localhost:8088](http://localhost:8088)\n",
    "\n",
    "---\n",
    "\n",
    "## üìÅ Step 10: Upload & Read Files from HDFS\n",
    "\n",
    "```bash\n",
    "echo \"Hello Hadoop\" > ~/test.txt\n",
    "hdfs dfs -mkdir /input\n",
    "hdfs dfs -put ~/test.txt /input/\n",
    "hdfs dfs -cat /input/test.txt\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Common Issues & Fixes\n",
    "\n",
    "### ‚ùå **Problem:** ResourceManager Fails with Java 17\n",
    "\n",
    "```text\n",
    "Caused by: java.lang.reflect.InaccessibleObjectException:\n",
    "module java.base does not \"opens java.lang\" to unnamed module\n",
    "```\n",
    "\n",
    "#### üß† Root Cause:\n",
    "- Java 17+ enforces strong module boundaries.\n",
    "- Hadoop uses reflection (Google Guice, Jetty), which is blocked without `--add-opens`.\n",
    "\n",
    "#### ‚úÖ Fix:\n",
    "Use **Java 8 instead**:\n",
    "\n",
    "```bash\n",
    "sudo apt install openjdk-8-jdk -y\n",
    "\n",
    "# Update hadoop-env.sh\n",
    "export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n",
    "\n",
    "# Restart services\n",
    "stop-yarn.sh\n",
    "stop-dfs.sh\n",
    "start-dfs.sh\n",
    "start-yarn.sh\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Optional (Advanced): Using Java 17 with `--add-opens` (not stable)\n",
    "\n",
    "In `hadoop-env.sh`:\n",
    "\n",
    "```bash\n",
    "export HADOOP_OPTS=\"--add-opens java.base/java.lang=ALL-UNNAMED\"\n",
    "```\n",
    "\n",
    "> Not guaranteed ‚Äî still may crash in other modules (Guice, Jetty, Jackson, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Other Issues\n",
    "\n",
    "| Symptom                                         | Likely Cause                       | Fix                                      |\n",
    "|--------------------------------------------------|------------------------------------|-------------------------------------------|\n",
    "| `ssh: connect to host localhost port 22: Connection refused` | SSH not installed or running      | `sudo apt install openssh-server`        |\n",
    "| ResourceManager UI not opening                  | Java version conflict              | Switch to Java 8                         |\n",
    "| File not found in HDFS                          | Incorrect path                     | `hdfs dfs -ls /` to verify paths         |\n",
    "| `Permission denied` error in file operations    | Wrong ownership or directory perms | Use proper user or set correct chmod     |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ You're All Set!\n",
    "\n",
    "You now have a full Hadoop 3.3.6 environment running on Ubuntu 20.04 with:\n",
    "\n",
    "- Java 8 compatibility\n",
    "- HDFS + YARN operational\n",
    "- UI dashboards available\n",
    "- CLI file operations working\n",
    "\n",
    "üéâ Happy Hadooping!\n",
    "\n"
   ],
   "id": "75918b00b10699ed"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
